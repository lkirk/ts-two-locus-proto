\documentclass[12pt]{article}

\usepackage[tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in]{geometry}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{cite}
\usepackage{amsmath}
\usepackage[bb=dsserif]{mathalpha}
\usepackage{svg}
\usepackage{relsize}
\usepackage{float}

\newcommand{\ccode}[1]{\mintinline{C}{#1}}
\usepackage[linesnumbered,lined,commentsnumbered]{algorithm2e}

\title{Two Locus General Statistics in tskit}
\author{Lloyd Kirk, Ragsdale Lab}

\begin{document}

\maketitle

\section{Background}

The intention of this document is to describe the changes to tskit in order to
provide a generalized framework for computing two-locus statistics for branches
and sites.

Some of the design proposal is taken from
\href{https://github.com/tskit-dev/tskit/pull/432}{GH-432} There has been a bit
of discussion on \href{https://github.com/tskit-dev/tskit/issues/1900}{GH-1900}
as well.

\section{Goal/Scope}

Ultimately, we intend to deprecate the LD calculator that is currently
implemented in tskit and implement a general framework for two-locus
statistics. This framework will create an interface for implementing new summary
statistics and implement the following features:

\begin{itemize}
  \item Statistics for sites with more than 1 mutation (multiallelic sites)
  \item Polarization, where applicable
  \item Branch Statistics
  \item LD Stats beyond $r^{2}$
\end{itemize}

\section{Implementation}
We have been trying to follow the general design pattern laid out in the C
api. Most of the initial design documentation will focus on creating the
necessary C interfaces to consume from the python api.

Some of this documentation is based on my (LKâ€™s) interpretation of the design
patterns in the existing code. I have no insight into how things might change
apart from the scattered TODOs in the code. In short, please feel free to
correct my assumptions/understandings.

\subsection{Outer Layer}

Each summary function we intend to implement will have an outer wrapper. For
example, the $r^2$ statistic, will have a \ccode{r2_summary_func} and a
corresponding \ccode{tsk_treeseq_r2}. These wrappers will call into
\ccode{tsk_treeseq_sample_count_stat}, which will check our sample sets and
produce a matrix of weights.

After these generic preparations (we will want to consider how we're handling
windows, see my proposal for the window design
\href{https://github.com/lkirk/ts-two-locus-proto/blob/main/notebooks/Windows.ipynb}{notebook}),
the code will call into \ccode{tsk_treeseq_general_stat}, which is the main
entrypoint for computing stats from tree sequences. Within
\ccode{tsk_treeseq_general_stat}, we will add two more conditions and a couple
more flags (\ccode{tsk_flags_t}).

\begin{minted}{c}
  bool stat_two_site = !!(options & TSK_STAT_TWO_SITE);
  bool stat_two_branch = !!(options & TSK_STAT_TWO_BRANCH);
\end{minted}

These flags will dispatch our entrypoints for computing two site/branch
statistics, allowing us to use the same entrypoint function for all site
statistics.

\subsection{Two Site Statistics}

We will provide a similar interface to computing
\ccode{tsk_treeseq_site_general_stat}, calling our version
\ccode{tsk_treeseq_two_site_general_stat}. This function will first populate an
array of all samples under each given mutation. Then, it will free some
intermediate data and allocate the output array. Finally, it will compute the
two site stat results and store them in the output array, normalized by the
normalization strategy for the summary function (see~\ref{section:normalization}).

\subsubsection{Bit Arrays}\label{section:bit_array}
Because we're storing samples instead of node counts, we must be efficient in
the way that we store samples.

The data structure that allows us to efficiently store samples is known in our
implementation as a \ccode{tsk_bit_array_t}. This is an array of unsigned 32-bit
integers, setting one bit for each sample. I've chosen to implement these as
32-bit integers instead of 64-bit integers because I assume that they will be
easier to auto-vectorize (unverified). The length of a bit array is the number
of possible items floor divided by the size of the unsigned integer type used in
the array (plus one more unsigned integer if the remainder of the division is
$>$ 1).

Another benefit of storing samples in this way is that we can efficiently
perform intersection operations on two arrays of integers. We achieve this by
performing a bitwise ``and'' on the chunks within the array (which seems to be
autovectorized by gcc -- see \href{https://godbolt.org/z/zd7qWGeKq}{godbolt}).

\begin{table}[H]
  \centering
  \begin{tabular}{ | c | c | c | }
    \hline
    a & 10000000000000000000000000000000 & 10000000000000000000000000000001 \\ \hline
    b & 11111111111111111111111111111111 & 11111111111111111111111111110000 \\
    \hline
  \end{tabular}
  \caption{
    \textbf{Examples of byte arrays}
    Case a represents an array for 64 samples, where sample 0, 32, 63 are
    present. Case b represents an array for 60 samples where all samples (0-59)
    are present. In both of these cases, there are two ``sample chunks'', which
    refer to the number of unsigned integers needed to store all of the samples.
  }
  \label{table:byte_array_example}
\end{table}

% Since we view the data as arrays of unsigned 32 bit integers, the data will
% appear as shown in the following example:

% \begin{table}[H]
%   \centering
%   \begin{tabular}{ | c | c | c | }
%     \hline
%     a & 1 & 2147483648 \\ \hline
%     b & 4294967295 & 4294967295 \\
%     \hline
%   \end{tabular}
%   \caption{
%     \textbf{Byte arrays as unsigned integers}
%     Case a represents an array for 64 samples, where sample 0, 32, 63 are
%     present. Case b represents an array for 60 samples where all samples (0-59)
%     are present.
%   }
%   \label{table:byte_array_int_example}
% \end{table}

\subsubsection{Walking the tree diffs}\label{section:node_counting}

In order to analyze the tree structure, we need to first build a small index of
the parents, right children, and left/right siblings of each node. This prepares
us to perform a preorder tree traversal in the next step of the algorithm. This
index is built as we apply the diffs for the next tree.

\subsubsection{Tree Traversal}

After a set of tree diffs has been applied, we have a valid tree to analyze. We
perform a single preorder traversal of the tree, where we track the path through
the tree for each mutation. This traversal does not start at the root of the
tree, but instead starts at the node above the uppermost mutation in the
tree. The paths are stored in another bit array of size (num node chunks
$\times$ num mutations). The number of node chunks is determined by the method
described in \ref{section:bit_array}. Memory usage scales by $\mathcal{O}(m
\times n)$, where m is the number of mutations and n is the number of nodes. We
only allocate this matrix per-tree, however, so the max memory usage is limited
to the maximum number of mutations on a single tree.

Following the paths down from the mutations to the samples allows us to see
which samples are child to the mutations in the tree. While we are tracing this
path, when we reach a sample node, we add the sample node to our intermediate
array of mutations $\times$ samples.

After we accumulate the samples under each mutation on our focal tree, we
enumerate the alleles for each site, using the same algorithm employed in
\ccode{get_allele_weights}, except that we're passing around sample bit arrays
instead of node counts. The result of our routine (named
\ccode{get_allele_samples}) gets stored in an output array that holds results
for all mutations and all samples (sample sets are not considered at this stage).

A simple benchmark of chromosome 1 of the sgdp dataset shows that applying all
tree diffs + tree traversals take ~7.5 seconds and about 750M of memory.

\paragraph{Tree Traversal Algorithm}
\mbox{} \\
\begin{algorithm}[H]
  \SetArgSty{textnormal}

  \emph{Allocate memory to hold this tree's data} \\
  node\_paths = calloc (num\_node\_chunks $\ast$ num\_mutations, sizeof ($\ast$
  node\_paths)) \\
  stack = malloc((1 + num\_samples + num\_edges) $\ast$ sizeof ($\ast$ stack)) \\

  \emph{} \\
  \emph{Initialize the stack with the parent of the uppermost node containing a mutation} \\
  stack.push(parent[top\_mut\_node]) \\

  \While{!stack.empty()}{

    node = stack.pop() \\
    \For{mut\_id = 0 \KwTo{} num\_mutations}{
      path = node\_paths[m] \\

      \emph{} \\
      \emph{If the current node contains this mutation, store it for the node} \\
      \If{node == mutation.node} {
        path[node] = 1 \\
        \If{node.is\_sample} {
          \emph{If the current node is a sample, store it for this mutation} \\
          mutation\_samples[node] = 1 \\
        }
      }

      \emph{} \\
      \emph{If this node's parent is in this mutation's path, then store the
        current node} \\
      \If{parent[node] in path} {
        path[node] = 1 \\
        \If{node.is\_sample} {
          \emph{If the current node is a sample, store it for this mutation} \\
          mutation\_samples[node] = 1 \\
        }
      }
    }

    \emph{} \\
    \emph{Continue the postorder traversal (handles polysomies)} \\
    u = right\_child[node] \\
    \While {u $\neq$ NULL} {
      stack.push(u) \\
      u = left\_sib[u] \\
    }
  }
\end{algorithm}

\subsection{Computing results}\label{section:computing_results}

Given our matrix of alleles $\times$ samples, two focal sites, and our sample
sets, we summarize the data into the desired statistics. We achieve this by
iterating over all pairs of allele states for the two sites (skipping the
ancestral allele state if polarized). For each pair, we compute the allele
weights ($w\_AB$, $w\_Ab$, and $w\_aB$), which get passed into the summary
function. After we compute the stat result from the summary function, we
normalize the statistic with the selected normalization function
(see~\ref{section:normalization} for more on normalization). Throughout this
process, we keep a running total for the pair of sites, storing the result in a
scalar value.

\paragraph{Algorithm for computing stat results}
\mbox{} \\
\begin{algorithm}[H]
  \SetArgSty{textnormal}

  \emph{Loop over pairs} \\
  first\_allele = 1 if polarized else 0 \\
  \For{allele\_left = first\_allele \KwTo{} num\_alleles\_left}{
    \For{allele\_right = first\_allele \KwTo{} num\_alleles\_right}{
      \emph{Find all A/B samples for alleles} \\
      A\_samples = allele\_samples[allele\_left] \\
      B\_samples = allele\_samples[allele\_right] \\
      AB\_samples = intersect(A\_samples, B\_samples) \\
      \For{k = 0 \KwTo{} num\_sample\_sets} {
        \emph{Intersect all A/B samples with the current sample set} \\
        sample\_set = sample\_sets[k] \\
        sample\_set\_A\_samples = intersect(A\_samples, sample\_set) \\
        sample\_set\_B\_samples = intersect(B\_samples, sample\_set) \\
        sample\_set\_AB\_samples = intersect(AB\_samples, sample\_set) \\

        weight[k] = (w\_AB, w\_Ab, w\_aB) \\
      }

      \emph{Both of these functions operate at the sample set level} \\
      stat\_result = stat\_func(weight, params) \\
      norm\_result = norm\_func(weight, params) \\

      \For{k = 0 \KwTo{} num\_sample\_sets} {
        \emph{Store running total for sample set result in the current pair} \\
        result[k] += stat\_result[k] * norm\_result[k] \\
      }
    }
  }
\end{algorithm}

\section{Summary Functions}

Table~\ref{table:summary_functions} provides an overview of the summary
functions that we intend to implement as site and branch statistics. The
normalization strategy is described in further detail in~\ref{section:normalization}.

\begin{table}[H]
  \begin{tabular}{llll} Statistic & Polarization & Normalization & Equation\\

    \hline $D$ & Polarized & Total & $D = f_{ab} - f_{a}f_{b}$ \\

    $D^{\prime}$ & Unpolarized & Haplotype Weighted & $D^{\prime} =
\frac{D}{D_{max}}$ \\

    $D^{2}$ & Unpolarized & Total & $D^{2} = D^{2}$ \\

    $D_{z}$ & Unpolarized & Total & $D_{z} = D (1 - 2 f_{a})(1-2f_{b})$ \\

    $\pi_{2}$ & Unpolarized & Total & $\pi_{2} = f_{a}f_{b}(1-f_{a})(1-f_{b})$
\\

    $r$ & Polarized & Total & $r =
\frac{D}{\sqrt{f_{a}f_{b}(1-f_{a})(1-f_{b})}}$ \\

    $r^{2}$ & Unpolarized & Haplotype Weighted & $r^{2} =
\frac{D^{2}}{f_{a}f_{b}(1-f_{a})(1-f_{b})}$ \\
  \end{tabular}
\label{table:summary_functions}
\end{table}
Where $D_{max}$ is defined as:

\[
  D_{max} = 
  \begin{cases}
    \min\{f_{a}(1-f_{b}),f_{b}(1-f_{b})\} & \text{if~}D>=0 \\
    \min\{f_{a}f_{b},(1-f_{b})(1-f_{b})\} & \text{otherwise}
  \end{cases}
\]

\subsection{Normalization} \label{section:normalization}
In our testing of summary functions, we found that the appropriate normalization
procedure can vary depending on the summary function. We've settled on three
normalization procedures: ``Haplotype Weighted'', ``Allele Frequency Weighted'',
and ``Total''.

\subsubsection{Hapltype Weighted}
In the ``Haplotype Weighted'' normalization method, we weight the statistics by
the frequency of its haplotype. This is necessary in ratio statistics, such as
$D^{\prime}$, $r$ and $r^{2}$.
\[
  \sum_{i=1}^{n}\sum_{j=1}^{m}p(A_{i}B_{j})F_{ij}
\]
where $F$ is the summary function and $p(A_{i}B_{j})$ is the frequency of
haplotype $A_{i}B_{j}$. This method can be found
in~\cite{zhao2007evaluation}.

\subsubsection{Allele Frequency Weighted}
In the ``Allele Frequency'' normalization method, we weight the statistics by
the product of the allele frequencies. We do not currently implement any
statistics with this normalization strategy, but felt it was useful to
implement, for the sake of completeness.
\[
  \sum_{i=1}^{n}\sum_{j=1}^{m}p(A_{i})p(B_{j})F_{ij}
\]
where $F$ is the summary function and $p(A_{i})p(B_{j})$ is the product of the
allele frequencies of the $A_{i}$ allele and the $B_{j}$ allele. This method is
described in~\cite{hedrick1987gametic}.

\subsubsection{Total}
In the ``Total'' normalization method, we simply divide by the number of
haplotypes. If we're using a non-ratio statistic, this is likely the desired
normalization stategy.
\[
  \frac{1}{(n-\mathbb{1}_{p}) (m-\mathbb{1}_{p})}\sum_{i=1}^{n}\sum_{j=1}^{m}F_{ij}
\]
where $\mathbb{1}_{p}$ is an indicator function conditioned on whether or not
our statistic is polarized, $n$ is the number of alleles in site $a$, and $m$ is
the number of alleles in site $b$.

\subsection{Evaluation}
To ensure the correctness of our implementation, we have devised a number of
test scenarios that will produce data at the theoretical limits of the
statistics we've implemented. Note that our validation is not exhaustive, but it
is a reasonable starting point for the purposes of verifying the correctness of
our normalization strategy and our C implementation. These results were first
produced with a python prototype, which we verified to be correct before using
it to validate the C code.

\subsubsection{Test cases}
Table~\ref{table:test_cases} enumerates the various test cases that we are using
to validate the correctness of our metrics. The subsequent sections refer to
test cases by name. Each test case is a two-site state matrix with 8-9
samples. Each number in our matricies represents the enumerated allelic state
for each sample. For example, in the corerlated case, sample 4 from site A has
allelic state number 2 and sample 0 on site A has the ancestral allele, allelic
state number 0.

\begin{table}[H]
  \centering
  \begin{tabular}{lc} Name & $\left(\begin{array}{cc} Site A Allele \\ Site B Allele
                                     \\ \end{array}\right)$ \\

    \hline
    Correlated & $\left(\begin{array}{ccccccccc}
                          0 & 1 & 1 & 0 & 2 & 2 & 1 & 0 & 1 \\
                          1 & 2 & 2 & 1 & 0 & 0 & 2 & 1 & 2 \\
                        \end{array}\right)$ \\
    Uncorrelated & $\left(\begin{array}{ccccccccc}
                            0 & 0 & 0 & 1 & 1 & 1 & 2 & 2 & 2 \\
                            0 & 1 & 2 & 0 & 1 & 2 & 0 & 1 & 2 \\
                          \end{array}\right)$ \\
    Correlated Biallelic & $\left(\begin{array}{cccccccc}
                                    0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
                                    0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
                                  \end{array}\right)$ \\
    Uncorrelated Biallelic & $\left(\begin{array}{cccccccc}
                                      0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
                                      1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\
                                    \end{array}\right)$ \\
    Repulsion Biallelic & $\left(\begin{array}{cccccccc}
                                   0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
                                   1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
                                 \end{array}\right)$ \\
  \end{tabular}
  \caption{
    \textbf{Test cases for validating statistics.}
    In each case, we have an A and B site, representing the two sites under
    consideration for computaiton of our statistics.
  }
  \label{table:test_cases}
\end{table}

\subsubsection{Results}
Here, we provide results for a few common statistics, exercising the diversity
of strategies for normalization and polarization. The biallelic cases are useful
for verifying the correctness of $r$ and $D$. We also obtain the expected
results in multiallelic cases in $r^2$.

\begin{table}[H]
  \begin{tabular}{lc} Name & Result \\
    \hline
    Correlated & $\left(\begin{array}{cc}
                          0.0556 & -0.01851 \\
                          -0.0185 & 0.0432 \\
                        \end{array}\right)$ \\
    Uncorrelated & $\left(\begin{array}{cc}
                            0.0556 & 0.0 \\
                            0.0 & 0.0556 \\
                          \end{array}\right)$ \\
    Correlated Biallelic & $\left(\begin{array}{cc}
                                    0.25 & 0.25 \\
                                    0.25 & 0.25 \\
                                  \end{array}\right)$ \\
    Uncorrelated Biallelic & $\left(\begin{array}{cc}
                                      0.25 & 0.0 \\
                                      0.0 & 0.25 \\
                                    \end{array}\right)$ \\
    Repulsion Biallelic & $\left(\begin{array}{cc}
                                   0.25 & -0.25 \\
                                   -0.25 & 0.25 \\
                                 \end{array}\right)$ \\
  \end{tabular}
  \caption{
    \textbf{Validation results for $D$}
    $D$ is polarized and weighted by the total number of haplotypes
  }
\end{table}

\begin{table}[H]
  \begin{tabular}{lc} Name & Result \\
    \hline
    Correlated & $\left(\begin{array}{cc}
                          0.2610 & -0.1221 \\
                          -0.1221 &  0.1838 \\
                        \end{array}\right)$ \\
    Correlated Biallelic & $\left(\begin{array}{cc}
                                    1. & 1. \\
                                    1. & 1. \\
                                  \end{array}\right)$ \\
    Uncorrelated & $\left(\begin{array}{cc}
                            0.25 & 0.   \\
                            0.   & 0.25 \\
                          \end{array}\right)$ \\
    Uncorrelated Biallelic & $\left(\begin{array}{cc}
                                      1. & 0. \\
                                      0. & 1. \\
                                    \end{array}\right)$ \\
    Repulsion Biallelic & $\left(\begin{array}{cc}
                                   1. & -1. \\
                                   -1. &  1. \\
                                 \end{array}\right)$ \\
  \end{tabular}
  \caption{
    \textbf{Validation results for $r$.}
    $r$ is polarized and weighted by the total number of haplotypes.
  }
\end{table}


\begin{table}[H]
  \begin{tabular}{lc} Name & Result \\
    \hline
    Correlated & $\left(\begin{array}{cc}
                          1. & 1. \\
                          1. & 1. \\
                        \end{array}\right)$ \\
    Correlated Biallelic & $\left(\begin{array}{cc}
                                    1. & 1. \\
                                    1. & 1. \\
                                  \end{array}\right)$ \\
    Uncorrelated & $\left(\begin{array}{cc}
                            1. & 0. \\
                            0. & 1. \\
                          \end{array}\right)$ \\
    Uncorrelated Biallelic & $\left(\begin{array}{cc}
                                      1. & 0. \\
                                      0. & 1. \\
                                    \end{array}\right)$ \\
    Repulsion Biallelic & $\left(\begin{array}{cc}
                                   1. & 1. \\
                                   1. & 1. \\
                                 \end{array}\right)$ \\
  \end{tabular}
  \caption{
    \textbf{Validation results for $r^2$.}
    $r^2$ is unpolarized and normalized by haplotype weighting.
  }
\end{table}








\bibliographystyle{plain}
\bibliography{references}

\end{document}